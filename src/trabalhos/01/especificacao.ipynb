{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a622311",
   "metadata": {},
   "source": [
    "\n",
    "# Trabalho 1: Diferenciação Automática com Grafos Computacionais\n",
    "\n",
    "## Informações Gerais\n",
    "\n",
    "- Data de Entrega: 29/06/2025\n",
    "- Pontuação: 10 pontos (+4 pontos extras)\n",
    "- O trabalho deve ser feito individualmente.\n",
    "- A entrega do trabalho deve ser realizada via sistema testr.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81844651",
   "metadata": {},
   "source": [
    "## Especificação\n",
    "\n",
    "⚠️ *Esta explicação assume que você leu e entendeu os slides sobre grafos computacionais.*\n",
    "\n",
    "O trabalho consiste em implementar um sistema de diferenciação automática usando grafos computacionais e utilizar este sistema para resolver um conjunto de problemas.\n",
    "\n",
    "Para isto, devem ser definidos um tipo Tensor para representar dados (similares aos arrays do numpy) e operações (e.g., soma, subtração, etc.) que geram tensores como saída. \n",
    "\n",
    "Sempre que uma operação é realizada, é armazenado no tensor de saída referências para os seus pais, isto é, os valores usados como entrada para a operação. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faacc1a1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19261d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Any\n",
    "from collections.abc import Iterable\n",
    "from abc import ABC, abstractmethod\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284f531",
   "metadata": {},
   "source": [
    "### Classe NameManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd00b34",
   "metadata": {},
   "source": [
    "A classe NameManager provê uma forma conveniente de dar nomes intuitivos para tensores que resultam de operações. A idéia é tornar mais fácil para o usuário das demais classes qual operação gerou qual tensor. Ela provê os seguintes métodos públicos: \n",
    "\n",
    "- reset(): reinicia o sistema de gestão de nomes.\n",
    "- new(<basename>: str): retorna um nome único a partir do nome de base passado como argumento. \n",
    "  \n",
    "Como indicado no exemplo abaixo da classe, a idéia geral é que uma sequência de operações é feita, os nomes dos tensores sejam os nomes das operações seguidos de um número. Se forem feitas 3 operações de soma e uma de multiplicação, seus tensores de saída terão os nomes \"add:0\", \"add:1\", \"add:2\" e \"prod:0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "162272a0",
   "metadata": {
    "tags": [
     "name_manager"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add:0\n",
      "in:0\n",
      "add:1\n",
      "add:2\n",
      "in:1\n",
      "prod:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NameManager:\n",
    "    _counts = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def reset():\n",
    "        NameManager._counts = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _count(name):\n",
    "        if name not in NameManager._counts:\n",
    "            NameManager._counts[name] = 0\n",
    "        count = NameManager._counts[name]\n",
    "        return count\n",
    "\n",
    "    @staticmethod\n",
    "    def _inc_count(name):\n",
    "        assert name in NameManager._counts, f'Name {name} is not registered.'\n",
    "        NameManager._counts[name] += 1\n",
    "\n",
    "    @staticmethod\n",
    "    def new(name: str):\n",
    "        count = NameManager._count(name)\n",
    "        tensor_name = f\"{name}:{count}\"\n",
    "        NameManager._inc_count(name)\n",
    "        return tensor_name\n",
    "\n",
    "# exemplo de uso\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('in'))\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('in'))\n",
    "print(NameManager.new('prod'))\n",
    "\n",
    "NameManager.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69485a9",
   "metadata": {},
   "source": [
    "### Classe Tensor\n",
    "\n",
    "Deve ser criada uma classe `Tensor` representando um array multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448496d7",
   "metadata": {
    "tags": [
     "tensor"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Tensor:\n",
    "    \"\"\"Classe representando um array multidimensional.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "    - _arr  (privado): dados internos do tensor como\n",
    "        um array do numpy com 2 dimensões (ver Regras)\n",
    "\n",
    "    - _parents (privado): lista de tensores que foram\n",
    "        usados como argumento para a operação que gerou o\n",
    "        tensor. Será vazia se o tensor foi inicializado com\n",
    "        valores diretamente. Por exemplo, se o tensor foi\n",
    "        resultado da operação a + b entre os tensores a e b,\n",
    "        _parents = [a, b].\n",
    "\n",
    "    - requires_grad (público): indica se devem ser\n",
    "        calculados gradientes para o tensor ou não.\n",
    "\n",
    "    - grad (público): Tensor representando o gradiente.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # Dados do tensor. Além dos tipos listados,\n",
    "                 # arr também pode ser do tipo Tensor.\n",
    "                 arr: Union[np.ndarray, list, numbers.Number, Any],\n",
    "                 # Entradas da operacao que gerou o tensor.\n",
    "                 # Deve ser uma lista de itens do tipo Tensor.\n",
    "                 parents: list[Any] = [],\n",
    "                 # se o tensor requer o calculo de gradientes ou nao\n",
    "                 requires_grad: bool = True,\n",
    "                 # nome do tensor\n",
    "                 name: str = '',\n",
    "                 # referência para um objeto do tipo Operation (ou\n",
    "                 # subclasse) indicando qual operação gerou este\n",
    "                 # tensor. Este objeto também possui um método\n",
    "                 # para calcular a derivada da operação.\n",
    "                 operation=None):\n",
    "        \"\"\"Construtor\n",
    "\n",
    "        O construtor deve permitir a criacao de tensores das seguintes formas:\n",
    "\n",
    "            # a partir de escalares\n",
    "            x = Tensor(3)\n",
    "\n",
    "            # a partir de listas\n",
    "            x = Tensor([1,2,3])\n",
    "\n",
    "            # a partir de arrays\n",
    "            x = Tensor(np.array([1,2,3]))\n",
    "\n",
    "            # a partir de outros tensores (construtor de copia)\n",
    "            x = Tensor(Tensor(np.array([1,2,3])))\n",
    "\n",
    "        Para isto, as seguintes regras devem ser obedecidas:\n",
    "\n",
    "        - Se o argumento arr não for um array do numpy,\n",
    "            ele deve ser convertido em um. Defina o dtype do\n",
    "            array como float de forma a permitir que NÃO seja\n",
    "            necessário passar constantes float como Tensor(3.0),\n",
    "            mas possamos criar um tensor apenas com Tensor(3).\n",
    "\n",
    "        - O atributo _arr deve ser uma matriz, isto é,\n",
    "            ter 2 dimensões (ver Regras).\n",
    "\n",
    "        - Se o argumento arr for um Tensor, ele deve ser\n",
    "            copiado (cuidado com cópias por referência).\n",
    "\n",
    "        - Se arr for um array do numpy com 1 dimensão,\n",
    "            ele deve ser convertido em uma matriz coluna.\n",
    "\n",
    "        - Se arr for um array do numpy com dimensão maior\n",
    "            que 2, deve ser lançada uma exceção.\n",
    "\n",
    "        - Tensores que não foram produzidos como resultado\n",
    "            de uma operação não têm pais nem operação.\n",
    "            Os nomes destes tensores devem seguir o formato in:3.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Reinicia o gradiente com zero\"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def numpy(self):\n",
    "        \"\"\"Retorna o array interno\"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Permite visualizar os dados do tensor como string\"\"\"\n",
    "        return f\"Tensor({self._arr}, name={self._name}, shape={self._arr.shape})\"\n",
    "\n",
    "    def backward(self, my_grad=None):\n",
    "        \"\"\"Método usado tanto iniciar o processo de\n",
    "        diferenciação automática, quanto por um filho\n",
    "        para enviar o gradiente do pai. No primeiro\n",
    "        caso, o argumento my_grad não será passado.\n",
    "        \"\"\"\n",
    "        # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c612fc",
   "metadata": {},
   "source": [
    "### Interface de  Operações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db44044",
   "metadata": {},
   "source": [
    "A classe abaixo define a interface que as operações devem implementar. Ela não precisa ser modificada, mas pode, caso queira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a19b73",
   "metadata": {
    "tags": [
     "op"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Op(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando as entradas e\n",
    "            retorna o tensor resultado. O método deve\n",
    "            garantir que o atributo parents do tensor\n",
    "            de saída seja uma lista de tensores.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna os gradientes dos pais em como tensores.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "        - back_grad: Derivada parcial em relação à saída\n",
    "            da operação backpropagada pelo filho.\n",
    "\n",
    "        - args: variaveis de entrada da operacao (pais)\n",
    "            como tensores.\n",
    "\n",
    "        - O nome dos tensores de gradiente devem ter o\n",
    "            nome da operacao seguido de '_grad'.\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89e386",
   "metadata": {},
   "source": [
    "### Implementação das Operações\n",
    "\n",
    "Operações devem herdar de `Op` e implementar os métodos `__call__` e `grad`.\n",
    "\n",
    "Pelo menos as seguintes operações devem ser implementadas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f7719",
   "metadata": {
    "tags": [
     "add"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Add(Op):\n",
    "    \"\"\"Add(a, b): a + b\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "add = Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb44e6",
   "metadata": {
    "tags": [
     "sub"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sub(Op):\n",
    "    \"\"\"Sub(a, b): a - b\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sub = Sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53df08",
   "metadata": {
    "tags": [
     "prod"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Prod(Op):\n",
    "    \"\"\"Prod(a, b): produto ponto a ponto de a e b ou produto escalar-tensor\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "prod = Prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3838a7",
   "metadata": {
    "tags": [
     "sin"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sin(Op):\n",
    "    \"\"\"seno element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sin = Sin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138cb8ef",
   "metadata": {
    "tags": [
     "cos"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Cos(Op):\n",
    "    \"\"\"cosseno element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "cos = Cos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eac52c",
   "metadata": {
    "tags": [
     "sum"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sum(Op):\n",
    "    \"\"\"Retorna a soma dos elementos do tensor\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "# ⚠️ vamos chamar de my_sum porque python ja possui uma funcao sum\n",
    "my_sum = Sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e098a39c",
   "metadata": {
    "tags": [
     "mean"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Mean(Op):\n",
    "    \"\"\"Retorna a média dos elementos do tensor\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "mean = Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37692879",
   "metadata": {
    "tags": [
     "square"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Square(Op):\n",
    "    \"\"\"Eleva cada elemento ao quadrado\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "square = Square()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6542807d",
   "metadata": {
    "tags": [
     "matmul"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class MatMul(Op):\n",
    "    \"\"\"MatMul(A, B): multiplicação de matrizes\n",
    "\n",
    "    C = A @ B\n",
    "    de/dA = de/dc @ B^T\n",
    "    de/dB = A^T @ de/dc\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "matmul = MatMul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a38e3",
   "metadata": {
    "tags": [
     "exp"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Exp(Op):\n",
    "    \"\"\"Exponenciação element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "exp = Exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc813e",
   "metadata": {
    "tags": [
     "relu"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class ReLU(Op):\n",
    "    \"\"\"ReLU element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "relu = ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae499275",
   "metadata": {
    "tags": [
     "sigmoid"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sigmoid(Op):\n",
    "    \"\"\"Sigmoid element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sigmoid = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce464ba",
   "metadata": {
    "tags": [
     "tanh"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Tanh(Op):\n",
    "    \"\"\"Tanh element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "tanh = Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a37eb",
   "metadata": {
    "tags": [
     "softmax"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Softmax(Op):\n",
    "    \"\"\"Softmax de um array de valores. Lembre-se que cada elemento do array influencia o resultado da função para todos os demais elementos.\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "softmax = Softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12256fd7",
   "metadata": {},
   "source": [
    "\n",
    "### ‼️ Regras e Pontos de Atenção‼️\n",
    "\n",
    "- Vamos fazer a hipótese simplificadora que Tensores devem ser sempre matrizes. Por exemplo, o escalar 2 deve ser armazado em `_arr` como a matriz `[[2]]`. De forma similar, a lista `[1, 2, 3]` deve ser armazenada em `_arr` como em uma matriz coluna.\n",
    "\n",
    "- Devem ser realizados `asserts` nas operações para garantir que os shapes dos operandos fazem sentido. Esta verificação também deve ser feita depois das operações que manipulam gradientes de tensores.\n",
    "\n",
    "- Devem ser respeitados os nomes dos atributos, métodos e classes para viabilizar os testes automáticos.\n",
    "\n",
    "- Gradientes devem ser calculados usando uma passada pelo grafo computacional.\n",
    "\n",
    "- Os gradientes devem ser somados e não substituídos nas chamadas de  backward. Isto vai permitir que os gradientes sejam acumulados entre amostras do dataset e que os resultados sejam corretos mesmo em caso de ramificações e junções no grafo computacional.\n",
    "\n",
    "- Lembre-se de zerar os gradientes após cada passo de gradient descent (atualização dos parâmetros).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc927248",
   "metadata": {},
   "source": [
    "## Testes Básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae08a8",
   "metadata": {},
   "source": [
    "Estes testes avaliam se a derivada da função está sendo calculada corretamente, mas em muitos casos **não** avaliam se os gradientes backpropagados estão sendo incorporados corretamente. Esta avaliação será feita nos problemas da próxima seção."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05318a9",
   "metadata": {},
   "source": [
    "Operador de Soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd20550",
   "metadata": {
    "tags": [
     "test_add"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[1.]\n",
      " [1.]\n",
      " [1.]], name=in_grad, shape=(3, 1))\n",
      "Tensor([[1.]\n",
      " [1.]\n",
      " [1.]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# add\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = add(a, b)\n",
    "d = add(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: matrizes coluna contendo 1\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac72b1a",
   "metadata": {},
   "source": [
    "Operador de Subtração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612377aa",
   "metadata": {
    "tags": [
     "test_sub"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[1.]\n",
      " [1.]\n",
      " [1.]], name=in_grad, shape=(3, 1))\n",
      "Tensor([[-1.]\n",
      " [-1.]\n",
      " [-1.]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# sub\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = sub(a, b)\n",
    "d = sub(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: matrizes coluna contendo 1 e -1\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c8e63",
   "metadata": {},
   "source": [
    "Operador de Produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60de82",
   "metadata": {
    "tags": [
     "test_prod"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[12.]\n",
      " [15.]\n",
      " [18.]], name=in_grad, shape=(3, 1))\n",
      "Tensor([[3.]\n",
      " [6.]\n",
      " [9.]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# prod\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = prod(a, b)\n",
    "d = prod(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: [12, 15, 18]^T\n",
    "print(a.grad)\n",
    "# esperado: [3, 6, 9]^T\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91e1c3",
   "metadata": {},
   "source": [
    "Operadores trigonométricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6185a989",
   "metadata": {
    "tags": [
     "test_sin_cos"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[-1.]\n",
      " [ 1.]\n",
      " [-1.]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# sin e cos\n",
    "\n",
    "a = Tensor([np.pi, 0, np.pi/2])\n",
    "b = sin(a)\n",
    "c = cos(a)\n",
    "d = my_sum(add(b, c))\n",
    "d.backward()\n",
    "\n",
    "# esperado: [-1, 1, -1]^T\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29f232",
   "metadata": {
    "tags": [
     "test_sum"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Sum\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = add(prod(a, 3.0), a)\n",
    "c = my_sum(b)\n",
    "c.backward()\n",
    "\n",
    "# esperado: [4, 4, 4, 4]^T\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943e71a",
   "metadata": {
    "tags": [
     "test_mean"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.25]\n",
      " [0.25]\n",
      " [0.25]\n",
      " [0.25]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Mean\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = mean(a)\n",
    "b.backward()\n",
    "\n",
    "# esperado: [0.25, 0.25, 0.25, 0.25]^T\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7dbd2c",
   "metadata": {
    "tags": [
     "test_square"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[9.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]], name=square:0, shape=(4, 1))\n",
      "Tensor([[6.]\n",
      " [2.]\n",
      " [0.]\n",
      " [4.]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Square\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = square(a)\n",
    "\n",
    "# esperado: [9, 1, 0, 4]^T\n",
    "print(b)\n",
    "\n",
    "b.backward()\n",
    "\n",
    "# esperado: [6, 2, 0, 4]\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2ead7",
   "metadata": {
    "tags": [
     "test_matmul"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[14.]\n",
      " [32.]\n",
      " [50.]], name=matmul:0, shape=(3, 1))\n",
      "Tensor([[1. 2. 3.]\n",
      " [1. 2. 3.]\n",
      " [1. 2. 3.]], name=in_grad, shape=(3, 3))\n",
      "Tensor([[12.]\n",
      " [15.]\n",
      " [18.]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# matmul\n",
    "\n",
    "W = Tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0]\n",
    "])\n",
    "\n",
    "v = Tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "z = matmul(W, v)\n",
    "\n",
    "# esperado: [14, 32, 50]^T\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "# esperado:\n",
    "# [1, 2, 3]\n",
    "# [1, 2, 3]\n",
    "# [1, 2, 3]\n",
    "print(W.grad)\n",
    "\n",
    "# esperado: [12, 15, 18]^T\n",
    "print(v.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706212d2",
   "metadata": {
    "tags": [
     "test_exp"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[ 2.71828183]\n",
      " [ 7.3890561 ]\n",
      " [20.08553692]], name=exp:0, shape=(3, 1))\n",
      "Tensor([[ 2.71828183]\n",
      " [ 7.3890561 ]\n",
      " [20.08553692]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# Exp\n",
    "\n",
    "v = Tensor([1.0, 2.0, 3.0])\n",
    "w = exp(v)\n",
    "\n",
    "# esperado: [2.718..., 7.389..., 20.085...]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [2.718..., 7.389..., 20.085...]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9510d010",
   "metadata": {
    "tags": [
     "test_relu"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [3.]], name=relu:0, shape=(4, 1))\n",
      "Tensor([[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Relu\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = relu(v)\n",
    "\n",
    "# esperado: [0, 0, 1, 3]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0, 0, 1, 1]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f0fbf8d",
   "metadata": {
    "tags": [
     "test_sigmoid"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.26894142]\n",
      " [0.5       ]\n",
      " [0.73105858]\n",
      " [0.95257413]], name=sigmoid:0, shape=(4, 1))\n",
      "Tensor([[0.19661193]\n",
      " [0.25      ]\n",
      " [0.19661193]\n",
      " [0.04517666]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = sigmoid(v)\n",
    "\n",
    "# esperado: [0.268.., 0.5, 0.731.., 0.952..]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0.196..., 0.25, 0.196..., 0.045...]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e867dec",
   "metadata": {
    "tags": [
     "test_tanh"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[-0.76159416]\n",
      " [ 0.        ]\n",
      " [ 0.76159416]\n",
      " [ 0.99505475]], name=tanh:0, shape=(4, 1))\n",
      "Tensor([[0.41997434]\n",
      " [1.        ]\n",
      " [0.41997434]\n",
      " [0.00986604]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Tanh\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = tanh(v)\n",
    "\n",
    "# esperado: [[-0.76159416, 0., 0.76159416, 0.99505475]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0.41997434, 1., 0.41997434, 0.00986604]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3235d",
   "metadata": {
    "tags": [
     "test_softmax"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.00381737]\n",
      " [0.13970902]\n",
      " [0.23034123]\n",
      " [0.62613238]], name=softmax:6, shape=(4, 1))\n",
      "MSE: Tensor([[0.36424932]], name=mean:7, shape=(1, 1))\n",
      "Tensor([[-0.00278095]\n",
      " [-0.02243068]\n",
      " [-0.02654377]\n",
      " [ 0.05175539]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "\n",
    "x = Tensor([-3.1, 0.5, 1.0, 2.0])\n",
    "y = softmax(x)\n",
    "\n",
    "# esperado: [0.00381737, 0.13970902, 0.23034123, 0.62613238]^T\n",
    "print(y)\n",
    "\n",
    "# como exemplo, calcula o MSE para um target vector\n",
    "diff = sub(y, [1, 0, 0, 0])\n",
    "sq = square(diff)\n",
    "a = mean(sq)\n",
    "\n",
    "# esperado: 0.36424932\n",
    "print(\"MSE:\", a)\n",
    "\n",
    "a.backward()\n",
    "\n",
    "# esperado: [-0.00278095, -0.02243068, -0.02654377, 0.05175539]^T\n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a3d9f",
   "metadata": {},
   "source": [
    "## Pontos Extras\n",
    "\n",
    "### Tarefas\n",
    "\n",
    "- **+2 pontos**: Utilizar sobrecarga de operadores para permitir que todas as operações disponíveis aos arrays do numpy possam ser realizadas com tensores, incluindo operações que envolvam broadcasting.\n",
    "  - Por exemplo, assumindo que a e b são tensores possivelmente com dimensões diferentes, devem ser possível realizar as operações a + 2, a * b, a @ b, a.max(), a.sum(axis=0).\n",
    "  - Para realizar esta atividade, os atributos da classe Tensor podem ser completamente modificados, mas deve ser provido um método backward para iniciar o backpropagation.\n",
    "  - Naturalmente, a regra de que tensores devem ser matrizes deve ser desconsiderada neste caso.\n",
    "\n",
    "- **+1 ponto**: Atualizar as classes para permitir derivadas de mais alta ordem (derivadas segundas, etc.).\n",
    "\n",
    "- **+1 ponto**: Entregar uma versão adicional do trabalho completo usando C/C++ e com foco em minimizar o tempo para realização das operações. Os casos de teste do sistema Testr também deverão ser replicados utilizando esta linguagem.\n",
    "\n",
    "### Regras\n",
    "\n",
    "- Só serão elegíveis para receber pontos extras os alunos que cumprirem 100% dos requisitos da parte principal do trabalho.\n",
    "\n",
    "- Para receber os pontos extras, deverá ser agendado um horário para uma entrevista individual que abordará tanto os códigos-fonte relativos aos pontos extras quanto à parte principal do trabalho (pode acontecer redução da pontuação da parte principal do trabalho).\n",
    "\n",
    "- Receberá os pontos extras quem responder corretamente às perguntas da entrevista. Não será atribuída pontuação parcial aos pontos extras.\n",
    "\n",
    "## Referências\n",
    "\n",
    "### Principais\n",
    "\n",
    "- [Build your own pytorch](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-1-Computation-Graphs/)\n",
    "- [Build your own Pytorch - 2: Backpropagation](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-2-Autograd/)\n",
    "- [Build your own PyTorch - 3: Training a Neural Network with self-made AD software](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-3-Build-Classifier/)\n",
    "- [Pytorch: A Gentle Introduction to torch.autograd](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "- [Automatic Differentiation with torch.autograd](https://docs.pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
    "\n",
    "### Secundárias\n",
    "\n",
    "- [Tom Roth: Building a computational graph: part 1](https://tomroth.dev/compgraph1/)\n",
    "- [Tom Roth: Building a computational graph: part 2](https://tomroth.dev/compgraph2/)\n",
    "- [Tom Roth: Building a computational graph: part 3](https://tomroth.dev/compgraph3/)\n",
    "- [Roger Grosse (Toronto) class on Automatic Differentiation](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf)\n",
    "- [Computational graphs and gradient flows](https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html)\n",
    "- [Colah Visual Blog: Backprop](https://colah.github.io/posts/2015-08-Backprop/)\n",
    "- [Towards Data Science: Automatic Differentiation (AutoDiff): A Brief Intro with Examples](https://towardsdatascience.com/automatic-differentiation-autodiff-a-brief-intro-with-examples-3f3d257ffe3b/)\n",
    "- [A Hands-on Introduction to Automatic Differentiation - Part 1](https://mostafa-samir.github.io/auto-diff-pt1/)\n",
    "- [Build Your own Deep Learning Framework - A Hands-on Introduction to Automatic Differentiation - Part 2](https://mostafa-samir.github.io/auto-diff-pt1/)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
