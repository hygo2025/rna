{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a622311",
   "metadata": {},
   "source": [
    "\n",
    "# Trabalho 1: Diferenciação Automática com Grafos Computacionais\n",
    "\n",
    "## Informações Gerais\n",
    "\n",
    "- Data de Entrega: 29/06/2025\n",
    "- Pontuação: 10 pontos (+4 pontos extras)\n",
    "- O trabalho deve ser feito individualmente.\n",
    "- A entrega do trabalho deve ser realizada via sistema testr.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81844651",
   "metadata": {},
   "source": [
    "## Especificação\n",
    "\n",
    "⚠️ *Esta explicação assume que você leu e entendeu os slides sobre grafos computacionais.*\n",
    "\n",
    "O trabalho consiste em implementar um sistema de diferenciação automática usando grafos computacionais e utilizar este sistema para resolver um conjunto de problemas.\n",
    "\n",
    "Para isto, devem ser definidos um tipo Tensor para representar dados (similares aos arrays do numpy) e operações (e.g., soma, subtração, etc.) que geram tensores como saída. \n",
    "\n",
    "Sempre que uma operação é realizada, é armazenado no tensor de saída referências para os seus pais, isto é, os valores usados como entrada para a operação. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faacc1a1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:45.927824Z",
     "start_time": "2025-06-14T21:31:45.315218Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install toolz",
   "id": "2591e652c30824c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: toolz in ./.venv/lib/python3.13/site-packages (1.0.0)\r\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "19261d1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:45.935891Z",
     "start_time": "2025-06-14T21:31:45.932070Z"
    }
   },
   "source": [
    "from typing import Optional, Union, Any\n",
    "from collections.abc import Iterable\n",
    "from abc import ABC, abstractmethod\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from toolz import pipe\n",
    "\n",
    "sns.set_style('whitegrid')"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "f284f531",
   "metadata": {},
   "source": [
    "### Classe NameManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd00b34",
   "metadata": {},
   "source": [
    "A classe NameManager provê uma forma conveniente de dar nomes intuitivos para tensores que resultam de operações. A idéia é tornar mais fácil para o usuário das demais classes qual operação gerou qual tensor. Ela provê os seguintes métodos públicos: \n",
    "\n",
    "- reset(): reinicia o sistema de gestão de nomes.\n",
    "- new(<basename>: str): retorna um nome único a partir do nome de base passado como argumento. \n",
    "  \n",
    "Como indicado no exemplo abaixo da classe, a idéia geral é que uma sequência de operações é feita, os nomes dos tensores sejam os nomes das operações seguidos de um número. Se forem feitas 3 operações de soma e uma de multiplicação, seus tensores de saída terão os nomes \"add:0\", \"add:1\", \"add:2\" e \"prod:0\"."
   ]
  },
  {
   "cell_type": "code",
   "id": "162272a0",
   "metadata": {
    "tags": [
     "name_manager"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:45.986325Z",
     "start_time": "2025-06-14T21:31:45.982127Z"
    }
   },
   "source": [
    "\n",
    "class NameManager:\n",
    "    _counts = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def reset():\n",
    "        NameManager._counts = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _count(name):\n",
    "        if name not in NameManager._counts:\n",
    "            NameManager._counts[name] = 0\n",
    "        count = NameManager._counts[name]\n",
    "        return count\n",
    "\n",
    "    @staticmethod\n",
    "    def _inc_count(name):\n",
    "        assert name in NameManager._counts, f'Name {name} is not registered.'\n",
    "        NameManager._counts[name] += 1\n",
    "\n",
    "    @staticmethod\n",
    "    def new(name: str):\n",
    "        count = NameManager._count(name)\n",
    "        tensor_name = f\"{name}:{count}\"\n",
    "        NameManager._inc_count(name)\n",
    "        return tensor_name\n",
    "\n",
    "# exemplo de uso\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('in'))\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('in'))\n",
    "print(NameManager.new('prod'))\n",
    "\n",
    "NameManager.reset()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add:0\n",
      "in:0\n",
      "add:1\n",
      "add:2\n",
      "in:1\n",
      "prod:0\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "e69485a9",
   "metadata": {},
   "source": [
    "### Classe Tensor\n",
    "\n",
    "Deve ser criada uma classe `Tensor` representando um array multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "id": "448496d7",
   "metadata": {
    "tags": [
     "tensor"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.040874Z",
     "start_time": "2025-06-14T21:31:46.032495Z"
    }
   },
   "source": [
    "\n",
    "from toolz import pipe\n",
    "\n",
    "InputArr = Union[np.ndarray, list, numbers.Number, Any]\n",
    "\n",
    "def standardize_tensor(arr: InputArr) -> np.ndarray:\n",
    "  match arr:\n",
    "    case Tensor():\n",
    "      return arr.numpy().copy()\n",
    "\n",
    "    case list() | numbers.Number():\n",
    "      return np.array(arr, dtype=float)\n",
    "\n",
    "    case np.ndarray():\n",
    "      return arr.astype(float).copy()\n",
    "\n",
    "    case _:\n",
    "      raise TypeError(f\"Tipo de dado não suportado: {type(arr)}\")\n",
    "\n",
    "def standardize_dimensions(arr: np.ndarray) -> np.ndarray:\n",
    "  match arr.ndim:\n",
    "    case 0:\n",
    "      return arr.reshape((1, 1))  # escalar\n",
    "    case 1:\n",
    "      return arr.reshape(-1, 1) # vetor coluna\n",
    "    case 2:\n",
    "      return arr\n",
    "    case _:\n",
    "        raise ValueError(f\"Dimensões não suportadas: {arr.ndim}. Máximo 2 dimensões.\")\n",
    "\n",
    "class Tensor:\n",
    "  def __init__(self,\n",
    "         arr: InputArr,\n",
    "         parents: list[Any] = None,\n",
    "         requires_grad: bool = True,\n",
    "         name: str = '',\n",
    "         operation=None):\n",
    "\n",
    "    self.grad = None\n",
    "    self._arr = pipe(arr, standardize_tensor, standardize_dimensions)\n",
    "    self.requires_grad = requires_grad\n",
    "    self._operation = operation\n",
    "    self._name = name\n",
    "    self._parents = parents or []\n",
    "\n",
    "\n",
    "\n",
    "  def __neg__(self):\n",
    "    negated_arr = -self.numpy()\n",
    "    return Tensor(negated_arr, requires_grad=False)\n",
    "\n",
    "  def zero_grad(self):\n",
    "    \"\"\"Reinicia o gradiente com zero\"\"\"\n",
    "    if self.requires_grad: #Aqui fala pa reiniciar o gradiente com zero, entretante esse grad deve ser um um tensor como no enunciado ou um np.array?\n",
    "      # self._grad = np.zeros_like(self._arr)\n",
    "      self.grad = Tensor(np.zeros_like(self._arr), requires_grad=False, name=f\"{self._name}_grad\")\n",
    "\n",
    "  def numpy(self) -> np.ndarray:\n",
    "    \"\"\"Retorna o array interno\"\"\"\n",
    "    return self._arr\n",
    "\n",
    "  def __repr__(self):\n",
    "    \"\"\"Permite visualizar os dados do tensor como string\"\"\"\n",
    "    return f\"Tensor({self._arr}, name={self._name}, shape={self._arr.shape})\"\n",
    "\n",
    "  def backward(self, my_grad: 'Tensor' =None):\n",
    "    \"\"\"Método usado tanto iniciar o processo de\n",
    "    diferenciação automática, quanto por um filho\n",
    "    para enviar o gradiente do pai. No primeiro\n",
    "    caso, o argumento my_grad não será passado.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ainda nao entendi pq eu nao iria querer o grad, acho que só é valido para o ultimo elemnto do grafo\n",
    "    if not self.requires_grad:\n",
    "        return\n",
    "\n",
    "    if my_grad is None: # Se for o final do grafo retona tudo 1\n",
    "        my_grad = Tensor(np.ones_like(self._arr), requires_grad=False)\n",
    "\n",
    "    if self.grad is None: # Se for o primeiro gradiente, inicializa, nao da da somar None com um Tensor\n",
    "        self.grad = my_grad\n",
    "    else:\n",
    "        self.grad._arr += my_grad.numpy() # Aqui acredito que ocorre o acumumulo do gradiente ponto a ponto\n",
    "\n",
    "    if self._operation:\n",
    "      # O professor explicou na aula em relacao aos parametros nao nomeados, acho que devo passar, pois na classe Op o metodo grad recebe args que sao os pais\n",
    "      parent_grads = self._operation.grad(self.grad, *self._parents) # faz o calculo da operacao, por exemplo da soma\n",
    "      for parent, parent_grad_arr in zip(self._parents, parent_grads):\n",
    "        parent.backward(Tensor(parent_grad_arr))\n"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "d2c612fc",
   "metadata": {},
   "source": [
    "### Interface de  Operações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db44044",
   "metadata": {},
   "source": [
    "A classe abaixo define a interface que as operações devem implementar. Ela não precisa ser modificada, mas pode, caso queira."
   ]
  },
  {
   "cell_type": "code",
   "id": "28a19b73",
   "metadata": {
    "tags": [
     "op"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.089741Z",
     "start_time": "2025-06-14T21:31:46.086634Z"
    }
   },
   "source": [
    "\n",
    "class Op(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando as entradas e\n",
    "            retorna o tensor resultado. O método deve\n",
    "            garantir que o atributo parents do tensor\n",
    "            de saída seja uma lista de tensores.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna os gradientes dos pais em como tensores.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "        - back_grad: Derivada parcial em relação à saída\n",
    "            da operação backpropagada pelo filho.\n",
    "\n",
    "        - args: variaveis de entrada da operacao (pais)\n",
    "            como tensores.\n",
    "\n",
    "        - O nome dos tensores de gradiente devem ter o\n",
    "            nome da operacao seguido de '_grad'.\n",
    "        \"\"\"\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.140437Z",
     "start_time": "2025-06-14T21:31:46.136688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import wraps\n",
    "from toolz import pipe\n",
    "\n",
    "def preprocess_op(arity: int):\n",
    "  \"\"\"\n",
    "  Decorador que pré-processa os argumentos para uma operação.\n",
    "  Checa aridade e garante que todos os args são Tensors.\n",
    "  https://book.pythontips.com/en/latest/decorators.html#nesting-a-decorator-within-a-function\n",
    "  \"\"\"\n",
    "  def decorator(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "      # checa aridade\n",
    "      if len(args) != arity:\n",
    "        raise ValueError(\n",
    "          f\"A operação {func.__name__} espera {arity} operandos, mas recebeu {len(args)}.\"\n",
    "        )\n",
    "\n",
    "      # Força tudo ser Tensor\n",
    "      processed_args = pipe(\n",
    "        args,\n",
    "        lambda op_args: map(\n",
    "          lambda arg: arg if isinstance(arg, Tensor) else Tensor(arg, requires_grad=False),\n",
    "          op_args\n",
    "        ),\n",
    "        list\n",
    "      )\n",
    "\n",
    "      # chama a função original\n",
    "      return func(self, *processed_args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "  return decorator"
   ],
   "id": "fa3c925e33447987",
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "9b89e386",
   "metadata": {},
   "source": [
    "### Implementação das Operações\n",
    "\n",
    "Operações devem herdar de `Op` e implementar os métodos `__call__` e `grad`.\n",
    "\n",
    "Pelo menos as seguintes operações devem ser implementadas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aa4f7719",
   "metadata": {
    "tags": [
     "add"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.186558Z",
     "start_time": "2025-06-14T21:31:46.183223Z"
    }
   },
   "source": [
    "\n",
    "class Add(Op):\n",
    "    \"\"\"Add(a, b): a + b\"\"\"\n",
    "    @preprocess_op(arity=2)\n",
    "    def __call__(self, t_a: Tensor, t_b: Tensor) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "        return Tensor(\n",
    "          arr=t_a.numpy() + t_b.numpy(),\n",
    "          parents=[t_a, t_b],\n",
    "          operation=self,\n",
    "          requires_grad=t_a.requires_grad or t_b.requires_grad,\n",
    "          name='add'\n",
    "        )\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "        return [back_grad, back_grad]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "add = Add()"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "05cb44e6",
   "metadata": {
    "tags": [
     "sub"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.243354Z",
     "start_time": "2025-06-14T21:31:46.237322Z"
    }
   },
   "source": [
    "\n",
    "class Sub(Op):\n",
    "  \"\"\"Sub(a, b): a - b\"\"\"\n",
    "  @preprocess_op(arity=2)\n",
    "  def __call__(self, t_a: Tensor, t_b: Tensor) -> Tensor:\n",
    "    \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    return Tensor(\n",
    "      arr=t_a.numpy() - t_b.numpy(),\n",
    "      parents=[t_a, t_b],\n",
    "      operation=self,\n",
    "      requires_grad=t_a.requires_grad or t_b.requires_grad,\n",
    "      name='sub'\n",
    "    )\n",
    "\n",
    "  def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "    \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "    return [back_grad, -back_grad]  # A derivada de a - b em relação a a é 1 e em relação a b é -1\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sub = Sub()"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "6f53df08",
   "metadata": {
    "tags": [
     "prod"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.302910Z",
     "start_time": "2025-06-14T21:31:46.296771Z"
    }
   },
   "source": [
    "\n",
    "class Prod(Op):\n",
    "  \"\"\"Prod(a, b): produto ponto a ponto de a e b ou produto escalar-tensor\"\"\"\n",
    "  @preprocess_op(arity=2)\n",
    "  def __call__(self, t_a: Tensor, t_b: Tensor) -> Tensor:\n",
    "    \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "    return Tensor(\n",
    "      arr=np.multiply(t_a.numpy(), t_b.numpy()),\n",
    "      parents=[t_a, t_b],\n",
    "      operation=self,\n",
    "      requires_grad=t_a.requires_grad or t_b.requires_grad,\n",
    "      name='prod'\n",
    "    )\n",
    "\n",
    "  def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "    \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "    # f(x) = u(x) * v(x)\n",
    "    # f(x) = u'(x) * v(x) + u(x) * v'(x)\n",
    "    a, b = args\n",
    "\n",
    "    # a é u(x)\n",
    "    # b é v(x)\n",
    "    # f(x) = a * b até aqui normal\n",
    "\n",
    "    # primeiro vou fazer a derivada em relação a `a`\n",
    "    # f(x) = a * b e `b` é constante\n",
    "    # f'(x) = b em relacao a a\n",
    "\n",
    "    # agora em relação a `b`\n",
    "    # f(x) = a * b e `a` é constante\n",
    "    # f'(x) = a em relacao a b\n",
    "\n",
    "    # agora é multiplicar pelo gradiante\n",
    "    grad_a = np.multiply(back_grad.numpy(), b.numpy()) # gradiente que vem do pai * a derivada f'(x) = b em relacao a a\n",
    "    grad_b = np.multiply(back_grad.numpy(), a.numpy()) # gradiente que vem do pai * a derivada f'(x) = a em relacao a b\n",
    "\n",
    "    return [Tensor(grad_a, requires_grad=back_grad.requires_grad),\n",
    "            Tensor(grad_b, requires_grad=back_grad.requires_grad)]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "prod = Prod()"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "8f3838a7",
   "metadata": {
    "tags": [
     "sin"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.350710Z",
     "start_time": "2025-06-14T21:31:46.347169Z"
    }
   },
   "source": [
    "\n",
    "class Sin(Op):\n",
    "  \"\"\"seno element-wise\"\"\"\n",
    "  @preprocess_op(arity=1)\n",
    "  def __call__(self, t_a: Tensor) -> Tensor:\n",
    "    \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "    return Tensor(\n",
    "      arr=np.sin(t_a.numpy()),\n",
    "      parents=[t_a],\n",
    "      operation=self,\n",
    "      requires_grad=t_a.requires_grad,\n",
    "      name='sin'\n",
    "    )\n",
    "\n",
    "  def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "    \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "    # f(x) = sin(x)\n",
    "    # f'(x) = cos(x)\n",
    "    # gradiente que vem do pai * a derivada f'(x) = cos(x)\n",
    "    a, = args # Args aqui é uma tupla de 1 elemento por isso tive de adicionar a `,`\n",
    "    grad_a = np.multiply(back_grad.numpy(), np.cos(a.numpy()))\n",
    "\n",
    "    return [Tensor(grad_a, requires_grad=back_grad.requires_grad)]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sin = Sin()"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "138cb8ef",
   "metadata": {
    "tags": [
     "cos"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.400257Z",
     "start_time": "2025-06-14T21:31:46.396756Z"
    }
   },
   "source": [
    "\n",
    "class Cos(Op):\n",
    "  \"\"\"cosseno element-wise\"\"\"\n",
    "  @preprocess_op(arity=1)\n",
    "  def __call__(self, t_a: Tensor) -> Tensor:\n",
    "    \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "    return Tensor(\n",
    "      arr=np.cos(t_a.numpy()),\n",
    "      parents=[t_a],\n",
    "      operation=self,\n",
    "      requires_grad=t_a.requires_grad,\n",
    "      name='cos'\n",
    "    )\n",
    "\n",
    "\n",
    "  def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "    \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "    # f(x) = cos(x)\n",
    "    # f'(x) = -sin(x)\n",
    "    # gradiente que vem do pai * a derivada f'(x) = -sin(x)\n",
    "    a, = args # Args aqui é uma tupla de 1 elemento por isso tive de adicionar a `,`\n",
    "    grad_a = np.multiply(back_grad.numpy(), -np.sin(a.numpy()))\n",
    "\n",
    "    return [Tensor(grad_a, requires_grad=back_grad.requires_grad)]\n",
    "\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "cos = Cos()"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "46eac52c",
   "metadata": {
    "tags": [
     "sum"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:35:26.234310Z",
     "start_time": "2025-06-14T21:35:26.226474Z"
    }
   },
   "source": [
    "\n",
    "class Sum(Op):\n",
    "  \"\"\"Retorna a soma dos elementos do tensor\"\"\"\n",
    "  @preprocess_op(arity=1)\n",
    "  def __call__(self, t_a: Tensor) -> Tensor:\n",
    "    \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "    return Tensor(\n",
    "      arr=np.sum(t_a.numpy()),\n",
    "      parents=[t_a],\n",
    "      operation=self,\n",
    "      requires_grad=t_a.requires_grad,\n",
    "      name='sum'\n",
    "    )\n",
    "\n",
    "  def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "    \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "    # f(x) = x1 + x2 + x3 + ... + xn\n",
    "    # f'(x) = 1 + 0 + 0 + ... + 0 em relação a x1\n",
    "    # para cada elemento do tensor, a derivada é 1\n",
    "\n",
    "    a, = args # Args aqui é uma tupla de 1 elemento por isso tive de adicionar a `,`\n",
    "\n",
    "    grad_scalar = back_grad.numpy().item()\n",
    "\n",
    "    # dai eu crio um novo vetor com o mesmo valor para cada elemento do tensor e a mesma forma\n",
    "    grad_arr = np.full_like(a.numpy(), fill_value=grad_scalar)\n",
    "\n",
    "    return [Tensor(grad_arr, requires_grad=back_grad.requires_grad)]\n",
    "\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "# ⚠️ vamos chamar de my_sum porque python ja possui uma funcao sum\n",
    "my_sum = Sum()"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "e098a39c",
   "metadata": {
    "tags": [
     "mean"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:40:43.993736Z",
     "start_time": "2025-06-14T21:40:43.989575Z"
    }
   },
   "source": [
    "\n",
    "class Mean(Op):\n",
    "  \"\"\"Retorna a média dos elementos do tensor\"\"\"\n",
    "  @preprocess_op(arity=1)\n",
    "  def __call__(self, t_a: Tensor) -> Tensor:\n",
    "    \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "    return Tensor(\n",
    "      arr=np.mean(t_a.numpy()),\n",
    "      parents=[t_a],\n",
    "      operation=self,\n",
    "      requires_grad=t_a.requires_grad,\n",
    "      name='mean'\n",
    "    )\n",
    "\n",
    "  def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "    \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "    a, = args # Args aqui é uma tupla de 1 elemento por isso tive de adicionar a `,`\n",
    "    # f(x) = (x1 + x2 + x3 + ... + xn) * 1/n\n",
    "    # f'(x) = 1/n + 0 + 0 + ... + 0 em relação a x1\n",
    "    # para cada elemento do tensor, a derivada é 1/n\n",
    "    n = a.numpy().size  # número de elementos no tensor\n",
    "    grad_scalar = back_grad.numpy().item() / n  # gradiente que vem do pai dividido pelo número de elementos\n",
    "    # dai eu crio um novo vetor com o mesmo valor para cada elemento do tensor e a mesma forma\n",
    "    grad_arr = np.full_like(a.numpy(), fill_value=grad_scalar)\n",
    "    return [Tensor(grad_arr, requires_grad=back_grad.requires_grad)]\n",
    "\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "mean = Mean()"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "id": "37692879",
   "metadata": {
    "tags": [
     "square"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:43:40.107362Z",
     "start_time": "2025-06-14T21:43:40.101975Z"
    }
   },
   "source": [
    "\n",
    "class Square(Op):\n",
    "  \"\"\"Eleva cada elemento ao quadrado\"\"\"\n",
    "  @preprocess_op(arity=1)\n",
    "  def __call__(self, t_a: Tensor) -> Tensor:\n",
    "    \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "    return Tensor(\n",
    "      arr=np.square(t_a.numpy()),\n",
    "      parents=[t_a],\n",
    "      operation=self,\n",
    "      requires_grad=t_a.requires_grad,\n",
    "      name='square'\n",
    "    )\n",
    "\n",
    "  def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "    \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "    a, = args # Args aqui é uma tupla de 1 elemento por isso tive de adicionar a `,`\n",
    "    # f(x) = x^2\n",
    "    # f'(x) = 2x\n",
    "    # para cada elemento do tensor, a derivada é 2x\n",
    "\n",
    "    result = 2 * a.numpy()  # calcula 2x para cada elemento do tensor\n",
    "    grad_arr = np.multiply(back_grad.numpy(), result)  # multiplica pelo gradiente que vem do pai\n",
    "\n",
    "    return [Tensor(grad_arr, requires_grad=back_grad.requires_grad)]\n",
    "\n",
    "\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "square = Square()"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "id": "6542807d",
   "metadata": {
    "tags": [
     "matmul"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.603033Z",
     "start_time": "2025-06-14T21:31:46.600222Z"
    }
   },
   "source": [
    "\n",
    "class MatMul(Op):\n",
    "    \"\"\"MatMul(A, B): multiplicação de matrizes\n",
    "\n",
    "    C = A @ B\n",
    "    de/dA = de/dc @ B^T\n",
    "    de/dB = A^T @ de/dc\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "matmul = MatMul()"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "c08a38e3",
   "metadata": {
    "tags": [
     "exp"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.650561Z",
     "start_time": "2025-06-14T21:31:46.647682Z"
    }
   },
   "source": [
    "\n",
    "class Exp(Op):\n",
    "    \"\"\"Exponenciação element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "exp = Exp()"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "1acc813e",
   "metadata": {
    "tags": [
     "relu"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.706846Z",
     "start_time": "2025-06-14T21:31:46.703705Z"
    }
   },
   "source": [
    "\n",
    "class ReLU(Op):\n",
    "    \"\"\"ReLU element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "relu = ReLU()"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "ae499275",
   "metadata": {
    "tags": [
     "sigmoid"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.763191Z",
     "start_time": "2025-06-14T21:31:46.760505Z"
    }
   },
   "source": [
    "\n",
    "class Sigmoid(Op):\n",
    "    \"\"\"Sigmoid element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sigmoid = Sigmoid()"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "6ce464ba",
   "metadata": {
    "tags": [
     "tanh"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.820478Z",
     "start_time": "2025-06-14T21:31:46.817579Z"
    }
   },
   "source": [
    "\n",
    "class Tanh(Op):\n",
    "    \"\"\"Tanh element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "tanh = Tanh()"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "f15a37eb",
   "metadata": {
    "tags": [
     "softmax"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.868214Z",
     "start_time": "2025-06-14T21:31:46.865074Z"
    }
   },
   "source": [
    "\n",
    "class Softmax(Op):\n",
    "    \"\"\"Softmax de um array de valores. Lembre-se que cada elemento do array influencia o resultado da função para todos os demais elementos.\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "softmax = Softmax()"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "id": "12256fd7",
   "metadata": {},
   "source": [
    "\n",
    "### ‼️ Regras e Pontos de Atenção‼️\n",
    "\n",
    "- Vamos fazer a hipótese simplificadora que Tensores devem ser sempre matrizes. Por exemplo, o escalar 2 deve ser armazado em `_arr` como a matriz `[[2]]`. De forma similar, a lista `[1, 2, 3]` deve ser armazenada em `_arr` como em uma matriz coluna.\n",
    "\n",
    "- Devem ser realizados `asserts` nas operações para garantir que os shapes dos operandos fazem sentido. Esta verificação também deve ser feita depois das operações que manipulam gradientes de tensores.\n",
    "\n",
    "- Devem ser respeitados os nomes dos atributos, métodos e classes para viabilizar os testes automáticos.\n",
    "\n",
    "- Gradientes devem ser calculados usando uma passada pelo grafo computacional.\n",
    "\n",
    "- Os gradientes devem ser somados e não substituídos nas chamadas de  backward. Isto vai permitir que os gradientes sejam acumulados entre amostras do dataset e que os resultados sejam corretos mesmo em caso de ramificações e junções no grafo computacional.\n",
    "\n",
    "- Lembre-se de zerar os gradientes após cada passo de gradient descent (atualização dos parâmetros).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc927248",
   "metadata": {},
   "source": [
    "## Testes Básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae08a8",
   "metadata": {},
   "source": [
    "Estes testes avaliam se a derivada da função está sendo calculada corretamente, mas em muitos casos **não** avaliam se os gradientes backpropagados estão sendo incorporados corretamente. Esta avaliação será feita nos problemas da próxima seção."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05318a9",
   "metadata": {},
   "source": [
    "Operador de Soma"
   ]
  },
  {
   "cell_type": "code",
   "id": "9fd20550",
   "metadata": {
    "tags": [
     "test_add"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.925021Z",
     "start_time": "2025-06-14T21:31:46.921247Z"
    }
   },
   "source": [
    "# add\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = add(a, b)\n",
    "d = add(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: matrizes coluna contendo 1\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[1.]\n",
      " [1.]\n",
      " [1.]], name=, shape=(3, 1))\n",
      "Tensor([[1.]\n",
      " [1.]\n",
      " [1.]], name=, shape=(3, 1))\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "id": "fac72b1a",
   "metadata": {},
   "source": [
    "Operador de Subtração"
   ]
  },
  {
   "cell_type": "code",
   "id": "612377aa",
   "metadata": {
    "tags": [
     "test_sub"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:46.984826Z",
     "start_time": "2025-06-14T21:31:46.981275Z"
    }
   },
   "source": [
    "# sub\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = sub(a, b)\n",
    "d = sub(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: matrizes coluna contendo 1 e -1\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[1.]\n",
      " [1.]\n",
      " [1.]], name=, shape=(3, 1))\n",
      "Tensor([[-1.]\n",
      " [-1.]\n",
      " [-1.]], name=, shape=(3, 1))\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "9c7c8e63",
   "metadata": {},
   "source": [
    "Operador de Produto"
   ]
  },
  {
   "cell_type": "code",
   "id": "dc60de82",
   "metadata": {
    "tags": [
     "test_prod"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:31:47.039298Z",
     "start_time": "2025-06-14T21:31:47.035319Z"
    }
   },
   "source": [
    "# prod\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = prod(a, b)\n",
    "d = prod(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: [12, 15, 18]^T\n",
    "print(a.grad)\n",
    "# esperado: [3, 6, 9]^T\n",
    "print(b.grad)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[12.]\n",
      " [15.]\n",
      " [18.]], name=, shape=(3, 1))\n",
      "Tensor([[3.]\n",
      " [6.]\n",
      " [9.]], name=, shape=(3, 1))\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "id": "5d91e1c3",
   "metadata": {},
   "source": [
    "Operadores trigonométricos"
   ]
  },
  {
   "cell_type": "code",
   "id": "6185a989",
   "metadata": {
    "tags": [
     "test_sin_cos"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:36:36.657288Z",
     "start_time": "2025-06-14T21:35:31.496537Z"
    }
   },
   "source": [
    "# sin e cos\n",
    "\n",
    "a = Tensor([np.pi, 0, np.pi/2])\n",
    "b = sin(a)\n",
    "c = cos(a)\n",
    "d = my_sum(add(b, c))\n",
    "d.backward()\n",
    "\n",
    "# esperado: [-1, 1, -1]^T\n",
    "print(a.grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[-2.3]\n",
      " [ 2.3]\n",
      " [-2.3]], name=, shape=(3, 1))\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29f232",
   "metadata": {
    "tags": [
     "test_sum"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Sum\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = add(prod(a, 3.0), a)\n",
    "c = my_sum(b)\n",
    "c.backward()\n",
    "\n",
    "# esperado: [4, 4, 4, 4]^T\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8943e71a",
   "metadata": {
    "tags": [
     "test_mean"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:40:47.724460Z",
     "start_time": "2025-06-14T21:40:47.720822Z"
    }
   },
   "source": [
    "# Mean\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = mean(a)\n",
    "b.backward()\n",
    "\n",
    "# esperado: [0.25, 0.25, 0.25, 0.25]^T\n",
    "print(a.grad)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.25]\n",
      " [0.25]\n",
      " [0.25]\n",
      " [0.25]], name=, shape=(4, 1))\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "id": "1c7dbd2c",
   "metadata": {
    "tags": [
     "test_square"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-14T21:43:43.511960Z",
     "start_time": "2025-06-14T21:43:43.508231Z"
    }
   },
   "source": [
    "# Square\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = square(a)\n",
    "\n",
    "# esperado: [9, 1, 0, 4]^T\n",
    "print(b)\n",
    "\n",
    "b.backward()\n",
    "\n",
    "# esperado: [6, 2, 0, 4]\n",
    "print(a.grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[9.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]], name=square, shape=(4, 1))\n",
      "Tensor([[6.]\n",
      " [2.]\n",
      " [0.]\n",
      " [4.]], name=, shape=(4, 1))\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2ead7",
   "metadata": {
    "tags": [
     "test_matmul"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[14.]\n",
      " [32.]\n",
      " [50.]], name=matmul:0, shape=(3, 1))\n",
      "Tensor([[1. 2. 3.]\n",
      " [1. 2. 3.]\n",
      " [1. 2. 3.]], name=in_grad, shape=(3, 3))\n",
      "Tensor([[12.]\n",
      " [15.]\n",
      " [18.]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# matmul\n",
    "\n",
    "W = Tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0]\n",
    "])\n",
    "\n",
    "v = Tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "z = matmul(W, v)\n",
    "\n",
    "# esperado: [14, 32, 50]^T\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "# esperado:\n",
    "# [1, 2, 3]\n",
    "# [1, 2, 3]\n",
    "# [1, 2, 3]\n",
    "print(W.grad)\n",
    "\n",
    "# esperado: [12, 15, 18]^T\n",
    "print(v.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706212d2",
   "metadata": {
    "tags": [
     "test_exp"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[ 2.71828183]\n",
      " [ 7.3890561 ]\n",
      " [20.08553692]], name=exp:0, shape=(3, 1))\n",
      "Tensor([[ 2.71828183]\n",
      " [ 7.3890561 ]\n",
      " [20.08553692]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# Exp\n",
    "\n",
    "v = Tensor([1.0, 2.0, 3.0])\n",
    "w = exp(v)\n",
    "\n",
    "# esperado: [2.718..., 7.389..., 20.085...]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [2.718..., 7.389..., 20.085...]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9510d010",
   "metadata": {
    "tags": [
     "test_relu"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [3.]], name=relu:0, shape=(4, 1))\n",
      "Tensor([[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Relu\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = relu(v)\n",
    "\n",
    "# esperado: [0, 0, 1, 3]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0, 0, 1, 1]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f0fbf8d",
   "metadata": {
    "tags": [
     "test_sigmoid"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.26894142]\n",
      " [0.5       ]\n",
      " [0.73105858]\n",
      " [0.95257413]], name=sigmoid:0, shape=(4, 1))\n",
      "Tensor([[0.19661193]\n",
      " [0.25      ]\n",
      " [0.19661193]\n",
      " [0.04517666]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = sigmoid(v)\n",
    "\n",
    "# esperado: [0.268.., 0.5, 0.731.., 0.952..]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0.196..., 0.25, 0.196..., 0.045...]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e867dec",
   "metadata": {
    "tags": [
     "test_tanh"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[-0.76159416]\n",
      " [ 0.        ]\n",
      " [ 0.76159416]\n",
      " [ 0.99505475]], name=tanh:0, shape=(4, 1))\n",
      "Tensor([[0.41997434]\n",
      " [1.        ]\n",
      " [0.41997434]\n",
      " [0.00986604]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Tanh\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = tanh(v)\n",
    "\n",
    "# esperado: [[-0.76159416, 0., 0.76159416, 0.99505475]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0.41997434, 1., 0.41997434, 0.00986604]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3235d",
   "metadata": {
    "tags": [
     "test_softmax"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.00381737]\n",
      " [0.13970902]\n",
      " [0.23034123]\n",
      " [0.62613238]], name=softmax:6, shape=(4, 1))\n",
      "MSE: Tensor([[0.36424932]], name=mean:7, shape=(1, 1))\n",
      "Tensor([[-0.00278095]\n",
      " [-0.02243068]\n",
      " [-0.02654377]\n",
      " [ 0.05175539]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "\n",
    "x = Tensor([-3.1, 0.5, 1.0, 2.0])\n",
    "y = softmax(x)\n",
    "\n",
    "# esperado: [0.00381737, 0.13970902, 0.23034123, 0.62613238]^T\n",
    "print(y)\n",
    "\n",
    "# como exemplo, calcula o MSE para um target vector\n",
    "diff = sub(y, [1, 0, 0, 0])\n",
    "sq = square(diff)\n",
    "a = mean(sq)\n",
    "\n",
    "# esperado: 0.36424932\n",
    "print(\"MSE:\", a)\n",
    "\n",
    "a.backward()\n",
    "\n",
    "# esperado: [-0.00278095, -0.02243068, -0.02654377, 0.05175539]^T\n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a3d9f",
   "metadata": {},
   "source": [
    "## Pontos Extras\n",
    "\n",
    "### Tarefas\n",
    "\n",
    "- **+2 pontos**: Utilizar sobrecarga de operadores para permitir que todas as operações disponíveis aos arrays do numpy possam ser realizadas com tensores, incluindo operações que envolvam broadcasting.\n",
    "  - Por exemplo, assumindo que a e b são tensores possivelmente com dimensões diferentes, devem ser possível realizar as operações a + 2, a * b, a @ b, a.max(), a.sum(axis=0).\n",
    "  - Para realizar esta atividade, os atributos da classe Tensor podem ser completamente modificados, mas deve ser provido um método backward para iniciar o backpropagation.\n",
    "  - Naturalmente, a regra de que tensores devem ser matrizes deve ser desconsiderada neste caso.\n",
    "\n",
    "- **+1 ponto**: Atualizar as classes para permitir derivadas de mais alta ordem (derivadas segundas, etc.).\n",
    "\n",
    "- **+1 ponto**: Entregar uma versão adicional do trabalho completo usando C/C++ e com foco em minimizar o tempo para realização das operações. Os casos de teste do sistema Testr também deverão ser replicados utilizando esta linguagem.\n",
    "\n",
    "### Regras\n",
    "\n",
    "- Só serão elegíveis para receber pontos extras os alunos que cumprirem 100% dos requisitos da parte principal do trabalho.\n",
    "\n",
    "- Para receber os pontos extras, deverá ser agendado um horário para uma entrevista individual que abordará tanto os códigos-fonte relativos aos pontos extras quanto à parte principal do trabalho (pode acontecer redução da pontuação da parte principal do trabalho).\n",
    "\n",
    "- Receberá os pontos extras quem responder corretamente às perguntas da entrevista. Não será atribuída pontuação parcial aos pontos extras.\n",
    "\n",
    "## Referências\n",
    "\n",
    "### Principais\n",
    "\n",
    "- [Build your own pytorch](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-1-Computation-Graphs/)\n",
    "- [Build your own Pytorch - 2: Backpropagation](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-2-Autograd/)\n",
    "- [Build your own PyTorch - 3: Training a Neural Network with self-made AD software](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-3-Build-Classifier/)\n",
    "- [Pytorch: A Gentle Introduction to torch.autograd](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "- [Automatic Differentiation with torch.autograd](https://docs.pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
    "\n",
    "### Secundárias\n",
    "\n",
    "- [Tom Roth: Building a computational graph: part 1](https://tomroth.dev/compgraph1/)\n",
    "- [Tom Roth: Building a computational graph: part 2](https://tomroth.dev/compgraph2/)\n",
    "- [Tom Roth: Building a computational graph: part 3](https://tomroth.dev/compgraph3/)\n",
    "- [Roger Grosse (Toronto) class on Automatic Differentiation](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf)\n",
    "- [Computational graphs and gradient flows](https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html)\n",
    "- [Colah Visual Blog: Backprop](https://colah.github.io/posts/2015-08-Backprop/)\n",
    "- [Towards Data Science: Automatic Differentiation (AutoDiff): A Brief Intro with Examples](https://towardsdatascience.com/automatic-differentiation-autodiff-a-brief-intro-with-examples-3f3d257ffe3b/)\n",
    "- [A Hands-on Introduction to Automatic Differentiation - Part 1](https://mostafa-samir.github.io/auto-diff-pt1/)\n",
    "- [Build Your own Deep Learning Framework - A Hands-on Introduction to Automatic Differentiation - Part 2](https://mostafa-samir.github.io/auto-diff-pt1/)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
